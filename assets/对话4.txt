
Model
ThinkingThoughts
Expand to view model thoughts

chevron_right
User
#!/usr/bin/env python3
"""
å®Œå…¨è‡ªåŠ¨åŒ–å‚æ•°è°ƒä¼˜è®­ç»ƒè„šæœ¬
ä½¿ç”¨ Optuna è´å¶æ–¯ä¼˜åŒ–è‡ªåŠ¨æœç´¢æœ€ä¼˜è¶…å‚æ•°
"""
import sys
import os
import pandas as pd
import numpy as np
from typing import Tuple, List, Dict, Any
from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix
import json
import pickle
import warnings
from datetime import datetime
import optuna
import akshare as ak
import xgboost as xgb
warnings.filterwarnings('ignore')
æ·»åŠ srcå’Œscriptsåˆ°è·¯å¾„
workspace_path = os.getenv("COZE_WORKSPACE_PATH", "/workspace/projects")
sys.path.insert(0, os.path.join(workspace_path, "src"))
sys.path.insert(0, os.path.join(workspace_path, "scripts"))
from stock_system.enhanced_features import EnhancedFeatureEngineer
class AutoTunedTrainer:
"""è‡ªåŠ¨åŒ–å‚æ•°è°ƒä¼˜è®­ç»ƒå™¨"""
code
Code
def __init__(self, config_path: str = "config/precision_priority_config.json"):
    self.config_path = config_path
    self.config = self._load_config()
    self.feature_engineer = EnhancedFeatureEngineer(config_path)
    self.best_model = None
    self.best_params = None
    self.feature_names = None
    self.study = None

def _convert_numpy_to_python(self, obj):
    """å°† numpy ç±»å‹è½¬æ¢ä¸ºåŸç”Ÿ Python ç±»å‹"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: self._convert_numpy_to_python(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [self._convert_numpy_to_python(item) for item in obj]
    else:
        return obj

def _load_config(self) -> Dict:
    """åŠ è½½é…ç½®"""
    config = {
        "data": {
            "start_date": "2020-01-01",
            "end_date": datetime.now().strftime("%Y-%m-%d"),
            "min_return_threshold": 0.04,
            "prediction_days": [3, 4, 5],
            "n_stocks": 150
        },
        "optuna": {
            "n_trials": 50,  # ä¼˜åŒ–è¯•éªŒæ¬¡æ•°
            "timeout": 3600,  # æœ€é•¿ä¼˜åŒ–æ—¶é—´ï¼ˆç§’ï¼‰
            "direction": "maximize",  # ä¼˜åŒ–æ–¹å‘
            "metric": "f1",  # ä¼˜åŒ–æŒ‡æ ‡: f1, precision, recall, auc
            "cv_folds": 3,  # äº¤å‰éªŒè¯æŠ˜æ•°
            "early_stopping_rounds": 50
        },
        "threshold": {
            "target_precision": 0.60,
            "threshold_range": [0.15, 0.45],
            "threshold_step": 0.01
        }
    }

    if os.path.exists(self.config_path):
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                external_config = json.load(f)
            config.update(external_config)
        except Exception as e:
            print(f"âš  åŠ è½½å¤–éƒ¨é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")

    return config

def get_stock_list(self, n_stocks: int = 150) -> List[str]:
    """è·å–è‚¡ç¥¨åˆ—è¡¨"""
    print("=" * 70)
    print(f"ã€æ­¥éª¤1ã€‘è·å–{n_stocks}åªè‚¡ç¥¨åˆ—è¡¨")
    print("=" * 70)

    try:
        stock_list = ak.stock_info_a_code_name()
        print(f"âœ“ è·å–åˆ° {len(stock_list)} åªAè‚¡è‚¡ç¥¨")

        # è¿‡æ»¤è‚¡ç¥¨
        stock_list = stock_list[~stock_list['name'].str.contains('ST|é€€|æš‚åœ', na=False)]
        print(f"âœ“ è¿‡æ»¤STã€é€€å¸‚å: {len(stock_list)} åª")

        stock_list = stock_list.head(n_stocks * 2)
        print(f"âœ“ é€‰å–å‰ {len(stock_list)} åªè‚¡ç¥¨ç”¨äºæ•°æ®é‡‡é›†")
        print()

        return stock_list['code'].tolist()[:n_stocks]

    except Exception as e:
        print(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
        return []

def collect_stock_data(self, stock_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """é‡‡é›†å•åªè‚¡ç¥¨å†å²æ•°æ®"""
    try:
        df = ak.stock_zh_a_hist(
            symbol=stock_code,
            period="daily",
            start_date=start_date.replace('-', ''),
            end_date=end_date.replace('-', ''),
            adjust="qfq"
        )

        df['stock_code'] = stock_code
        df['date'] = pd.to_datetime(df['æ—¥æœŸ'])
        df = df.drop(columns=['æ—¥æœŸ'])
        df = df.set_index('date')

        # é‡å‘½ååˆ—
        df = df.rename(columns={
            'å¼€ç›˜': 'open',
            'æ”¶ç›˜': 'close',
            'æœ€é«˜': 'high',
            'æœ€ä½': 'low',
            'æˆäº¤é‡': 'volume',
            'æˆäº¤é¢': 'amount',
            'æŒ¯å¹…': 'amplitude',
            'æ¶¨è·Œå¹…': 'pct_change',
            'æ¶¨è·Œé¢': 'change_amount',
            'æ¢æ‰‹ç‡': 'turnover_rate'
        })

        return df

    except Exception as e:
        print(f"  âš  {stock_code} æ•°æ®è·å–å¤±è´¥: {e}")
        return None

def split_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """æŒ‰æ—¶é—´åºåˆ—åˆ’åˆ†æ•°æ®é›†"""
    print("=" * 70)
    print("ã€æ­¥éª¤2ã€‘æ—¶é—´åºåˆ—åˆ’åˆ†æ•°æ®é›†")
    print("=" * 70)

    df = df.sort_index()

    n = len(df)
    n_train = int(n * 0.60)
    n_val = int(n * 0.20)

    train_df = df.iloc[:n_train].copy()
    val_df = df.iloc[n_train:n_train + n_val].copy()
    test_df = df.iloc[n_train + n_val:].copy()

    print(f"âœ“ æ•°æ®é›†åˆ’åˆ†ï¼ˆæŒ‰æ—¶é—´ï¼‰:")
    print(f"  è®­ç»ƒé›†: {len(train_df)}æ¡ ({train_df.index.min()} è‡³ {train_df.index.max()})")
    print(f"  éªŒè¯é›†: {len(val_df)}æ¡ ({val_df.index.min()} è‡³ {val_df.index.max()})")
    print(f"  æµ‹è¯•é›†: {len(test_df)}æ¡ ({test_df.index.min()} è‡³ {test_df.index.max()})")
    print()

    return train_df, val_df, test_df

def create_features_only(self, df: pd.DataFrame) -> pd.DataFrame:
    """è®¡ç®—ç‰¹å¾"""
    print("=" * 70)
    print("ã€æ­¥éª¤3ã€‘è®¡ç®—ç‰¹å¾ï¼ˆå¢å¼ºç‰ˆï¼‰")
    print("=" * 70)

    required_cols = ['open', 'high', 'low', 'close', 'volume', 'stock_code']
    for col in required_cols:
        if col not in df.columns:
            raise ValueError(f"æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {col}")

    df_features = self.feature_engineer.create_all_features(df)

    print(f"âœ“ ç‰¹å¾è®¡ç®—å®Œæˆ:")
    print(f"  æ ·æœ¬æ•°: {len(df_features)}")
    print(f"  ç‰¹å¾æ•°: {len(self.feature_engineer.get_feature_names())}")
    print()

    return df_features

def create_labels_separately(self, train_df: pd.DataFrame,
                               val_df: pd.DataFrame,
                               test_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """åˆ›å»ºæ ‡ç­¾ï¼ˆç›®æ ‡ï¼šâ‰¥4%ï¼Œ3-5å¤©ï¼‰"""
    print("=" * 70)
    print("ã€æ­¥éª¤4ã€‘åˆ›å»ºæ ‡ç­¾ï¼ˆç›®æ ‡ï¼šâ‰¥4%ï¼Œ3-5å¤©ï¼‰")
    print("=" * 70)

    min_return = self.config['data']['min_return_threshold']
    prediction_days = self.config['data']['prediction_days']

    for name, df in [('è®­ç»ƒé›†', train_df), ('éªŒè¯é›†', val_df), ('æµ‹è¯•é›†', test_df)]:
        # è®¡ç®—æœªæ¥3-5å¤©çš„æ¶¨å¹…
        for days in prediction_days:
            df[f'future_return_{days}d'] = df.groupby('stock_code')['close'].pct_change(days).shift(-days)

        # æ ‡ç­¾ï¼š3-5å¤©å†…ä»»æ„ä¸€å¤©æ¶¨å¹…â‰¥4%
        future_returns = [f'future_return_{days}d' for days in prediction_days]
        df['max_future_return'] = df[future_returns].max(axis=1)
        df['label'] = (df['max_future_return'] >= min_return).astype(int)

        # ç§»é™¤æ‰€æœ‰æœªæ¥æ•°æ®åˆ—
        future_cols = [col for col in df.columns if 'future_return' in col or col == 'max_future_return']
        df.drop(columns=future_cols, inplace=True)

        # ç§»é™¤æ— æ³•è®¡ç®—çš„æ ·æœ¬
        df.dropna(subset=['label'], inplace=True)

        print(f"{name}:")
        print(f"  æ ·æœ¬æ•°: {len(df)}")
        print(f"  æ­£æ ·æœ¬ï¼ˆâ‰¥{min_return * 100:.0f}%ï¼‰: {df['label'].sum()} ({df['label'].mean():.2%})")

    print()
    return train_df, val_df, test_df

def extract_features_and_labels(self, train_df: pd.DataFrame,
                                  val_df: pd.DataFrame,
                                  test_df: pd.DataFrame) -> Tuple:
    """æå–ç‰¹å¾å’Œæ ‡ç­¾"""
    print("=" * 70)
    print("ã€æ­¥éª¤5ã€‘æå–ç‰¹å¾å’Œæ ‡ç­¾")
    print("=" * 70)

    try:
        feature_names = self.feature_engineer.get_feature_names()
    except Exception as e:
        print(f"ERROR: è·å–feature_nameså¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

    try:
        exclude_cols = [
            'open', 'high', 'low', 'close', 'volume',
            'amount', 'amplitude', 'pct_change', 'change_amount',
            'turnover_rate', 'stock_code', 'è‚¡ç¥¨ä»£ç ',
            'future_return', 'max_future_return', 'label',
            'date', 'returns', 'daily_return', 'æ—¥æœŸ'
        ]

        available_features = [col for col in feature_names if col not in exclude_cols]
        existing_features = [col for col in available_features if col in train_df.columns]

        # æ£€æŸ¥æœªæ¥æ•°æ®åˆ—
        future_keywords = ['future', 'return_', 'max_return']
        for col in existing_features:
            if any(keyword in col.lower() for keyword in future_keywords):
                raise ValueError(f"å‘ç°æœªæ¥æ•°æ®åˆ—: {col}ï¼Œå¿…é¡»å®Œå…¨ç§»é™¤ï¼")

        print(f"DEBUG: å¼€å§‹æå–ç‰¹å¾çŸ©é˜µ...")
        X_train = train_df[existing_features].values
        X_val = val_df[existing_features].values
        X_test = test_df[existing_features].values

        y_train = train_df['label'].values
        y_val = val_df['label'].values
        y_test = test_df['label'].values

        print(f"âœ“ ç‰¹å¾å’Œæ ‡ç­¾æå–å®Œæˆ:")
        print(f"  ç‰¹å¾æ•°: {len(existing_features)}")
        print(f"  è®­ç»ƒé›†: {X_train.shape}, æ­£æ ·æœ¬: {y_train.sum()} ({y_train.mean():.2%})")
        print(f"  éªŒè¯é›†: {X_val.shape}, æ­£æ ·æœ¬: {y_val.sum()} ({y_val.mean():.2%})")
        print(f"  æµ‹è¯•é›†: {X_test.shape}, æ­£æ ·æœ¬: {y_test.sum()} ({y_test.mean():.2%})")
        print()

    except Exception as e:
        print(f"ERROR: æå–ç‰¹å¾å’Œæ ‡ç­¾å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

    self.feature_names = available_features
    return X_train, y_train, X_val, y_val, X_test, y_test

def define_search_space(self, trial: optuna.Trial) -> Dict[str, Any]:
    """
    å®šä¹‰ Optuna å‚æ•°æœç´¢ç©ºé—´
    """
    params = {
        'learning_rate': trial.suggest_float('learning_rate', 0.003, 0.05, log=True),
        'max_depth': trial.suggest_int('max_depth', 3, 8),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10, log=True),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 5, log=True),  # ä¿®å¤ï¼šlow å¿…é¡» > 0 å½“ log=True
        'gamma': trial.suggest_float('gamma', 0.0, 5),
        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 5.0),
        'n_estimators': trial.suggest_int('n_estimators', 300, 1500),
        'random_state': 42,
        'eval_metric': 'logloss',
        'objective': 'binary:logistic',
        'tree_method': 'hist'
    }

    return params

def objective(self, trial: optuna.Trial,
              X_train: np.ndarray, y_train: np.ndarray,
              X_val: np.ndarray, y_val: np.ndarray) -> float:
    """
    Optuna ä¼˜åŒ–ç›®æ ‡å‡½æ•°
    ä½¿ç”¨éªŒè¯é›†è¯„ä¼°æ¨¡å‹æ€§èƒ½
    """
    try:
        # ä»è¯•éªŒä¸­é‡‡æ ·å‚æ•°
        params = self.define_search_space(trial)

        # è®¡ç®—æ ·æœ¬æƒé‡
        scale_pos_weight = params['scale_pos_weight']
        sample_weight = np.where(y_train == 1, scale_pos_weight, 1.0)

        # åˆ›å»ºæ¨¡å‹
        model = xgb.XGBClassifier(**params)

        # è®­ç»ƒæ¨¡å‹ï¼ˆXGBoost 3.x ä½¿ç”¨ early_stopping_rounds å‚æ•°ï¼‰
        model.fit(
            X_train, y_train,
            sample_weight=sample_weight,
            eval_set=[(X_val, y_val)],
            verbose=False
        )

        # é¢„æµ‹
        y_pred_proba = model.predict_proba(X_val)[:, 1]

        # ä¼˜åŒ–ç›®æ ‡ï¼ˆä»é…ç½®ä¸­è¯»å–ï¼‰
        metric = self.config['optuna']['metric']

        if metric == 'f1':
            # ä¼˜åŒ– F1 åˆ†æ•°ï¼ˆéœ€è¦å…ˆæ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼ï¼‰
            best_f1 = 0
            for threshold in np.arange(0.15, 0.45, 0.01):
                y_pred = (y_pred_proba >= threshold).astype(int)
                if y_pred.sum() > 0:
                    f1 = f1_score(y_val, y_pred)
                    if f1 > best_f1:
                        best_f1 = f1
            return best_f1

        elif metric == 'precision':
            # ä¼˜åŒ–ç²¾ç¡®ç‡
            best_precision = 0
            for threshold in np.arange(0.15, 0.45, 0.01):
                y_pred = (y_pred_proba >= threshold).astype(int)
                if y_pred.sum() > 0:
                    precision = precision_score(y_val, y_pred)
                    if precision > best_precision:
                        best_precision = precision
            return best_precision

        elif metric == 'auc':
            # ä¼˜åŒ– AUC
            return roc_auc_score(y_val, y_pred_proba)

        else:
            # é»˜è®¤ä¼˜åŒ– F1
            best_f1 = 0
            for threshold in np.arange(0.15, 0.45, 0.01):
                y_pred = (y_pred_proba >= threshold).astype(int)
                if y_pred.sum() > 0:
                    f1 = f1_score(y_val, y_pred)
                    if f1 > best_f1:
                        best_f1 = f1
            return best_f1

    except Exception as e:
        # è¿”å›æå°å€¼ä½œä¸ºæƒ©ç½š
        print(f"  âš  è¯•éªŒå¤±è´¥: {e}")
        return -1e6

def optimize_threshold(self, y_val: np.ndarray, y_val_pred_proba: np.ndarray) -> float:
    """ä¼˜åŒ–å†³ç­–é˜ˆå€¼"""
    print("=" * 70)
    print("ã€æ­¥éª¤7ã€‘ä¼˜åŒ–å†³ç­–é˜ˆå€¼ï¼ˆç›®æ ‡ç²¾ç¡®ç‡â‰¥60%ï¼‰")
    print("=" * 70)

    target_precision = self.config['threshold']['target_precision']
    threshold_range = self.config['threshold']['threshold_range']
    threshold_step = self.config['threshold']['threshold_step']

    thresholds = np.arange(threshold_range[0], threshold_range[1], threshold_step)
    best_threshold = threshold_range[0]
    best_f1 = 0

    print(f"{'é˜ˆå€¼':<8} {'ç²¾ç¡®ç‡':<10} {'å¬å›ç‡':<10} {'F1':<10} {'é¢„æµ‹æ•°'}")
    print("-" * 60)

    for threshold in thresholds:
        y_pred = (y_val_pred_proba >= threshold).astype(int)

        if y_pred.sum() > 0:
            precision = precision_score(y_val, y_pred, zero_division=0)
            recall = recall_score(y_val, y_pred, zero_division=0)
            f1 = f1_score(y_val, y_pred, zero_division=0)

            if precision >= target_precision and f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold

            # æ‰“å°éƒ¨åˆ†é˜ˆå€¼
            if abs(threshold - 0.25) < 0.02 or abs(threshold - best_threshold) < 0.01:
                print(f"{threshold:.2f}     {precision:.2%}      {recall:.2%}      {f1:.3f}     {y_pred.sum()}")

    print(f"\nâœ“ æœ€ä¼˜é˜ˆå€¼: {best_threshold:.2f} (ç²¾ç¡®ç‡ç›®æ ‡: {target_precision:.0%})")
    print()
    return best_threshold

def evaluate_model(self, X_test: np.ndarray, y_test: np.ndarray,
                   threshold: float = 0.5) -> Dict:
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print("=" * 70)
    print("ã€æ­¥éª¤8ã€‘æ¨¡å‹æ€§èƒ½è¯„ä¼°ï¼ˆæµ‹è¯•é›†ï¼‰")
    print("=" * 70)

    y_pred_proba = self.best_model.predict_proba(X_test)[:, 1]
    y_pred = (y_pred_proba >= threshold).astype(int)

    metrics = {
        'accuracy': (y_pred == y_test).mean(),
        'precision': precision_score(y_test, y_pred, zero_division=0),
        'recall': recall_score(y_test, y_pred, zero_division=0),
        'f1': f1_score(y_test, y_pred, zero_division=0),
        'auc': roc_auc_score(y_test, y_pred_proba),
        'n_predictions': y_pred.sum(),
        'n_positive_samples': y_test.sum()
    }

    cm = confusion_matrix(y_test, y_pred)

    print(f"âœ“ æ¨¡å‹æ€§èƒ½æŒ‡æ ‡ï¼ˆé˜ˆå€¼={threshold:.2f})")
    print(f"  å‡†ç¡®ç‡: {metrics['accuracy']:.2%}")
    print(f"  ç²¾ç¡®ç‡: {metrics['precision']:.2%}")
    print(f"  å¬å›ç‡: {metrics['recall']:.2%}")
    print(f"  F1åˆ†æ•°: {metrics['f1']:.3f}")
    print(f"  AUC: {metrics['auc']:.4f}")
    print(f"\nâœ“ æ··æ·†çŸ©é˜µ:")
    print(f"  é¢„æµ‹ä¸ºè´Ÿï¼ˆå®é™…è´Ÿæ ·æœ¬: {cm[0, 0]}, å®é™…æ­£æ ·æœ¬: {cm[1, 0]})")
    print(f"  é¢„æµ‹ä¸ºæ­£ï¼ˆå®é™…è´Ÿæ ·æœ¬: {cm[0, 1]}, å®é™…æ­£æ ·æœ¬: {cm[1, 1]})")
    print(f"\nâœ“ é¢„æµ‹ç»Ÿè®¡:")
    print(f"  é¢„æµ‹ä¸ºæ­£çš„æ ·æœ¬æ•°: {metrics['n_predictions']}")
    print(f"  å®é™…æ­£æ ·æœ¬æ•°: {metrics['n_positive_samples']}")
    print()

    return metrics

def save_model(self, threshold: float, metrics: Dict):
    """ä¿å­˜æ¨¡å‹å’Œé…ç½®"""
    print("=" * 70)
    print("ã€æ­¥éª¤9ã€‘ä¿å­˜æ¨¡å‹å’Œé…ç½®")
    print("=" * 70)

    # åˆ›å»ºæ¨¡å‹ç›®å½•
    model_dir = os.path.join(workspace_path, "assets", "models")
    os.makedirs(model_dir, exist_ok=True)

    # ä¿å­˜æ¨¡å‹
    model_path = os.path.join(model_dir, "auto_tuned_model.pkl")
    with open(model_path, 'wb') as f:
        pickle.dump({
            'model': self.best_model,
            'feature_names': self.feature_names,
            'feature_engineer': self.feature_engineer
        }, f)

    print(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")

    # ä¿å­˜å…ƒæ•°æ®ï¼ˆè½¬æ¢ numpy ç±»å‹ä¸ºåŸç”Ÿ Python ç±»å‹ï¼‰
    metadata = {
        'model_name': 'auto_tuned',
        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'feature_count': len(self.feature_names),
        'best_params': self.best_params,
        'decision_threshold': float(threshold),
        'metrics': self._convert_numpy_to_python(metrics),
        'config': self.config,
        'optimization': {
            'n_trials': self.config['optuna']['n_trials'],
            'best_score': float(self.study.best_value) if self.study else None,
            'metric': self.config['optuna']['metric']
        }
    }

    metadata_path = os.path.join(model_dir, "auto_tuned_metadata.json")
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    print(f"âœ“ å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")
    print()

    # ä¿å­˜ Optuna ç ”ç©¶ç»“æœ
    if self.study:
        study_path = os.path.join(model_dir, "optuna_study.pkl")
        with open(study_path, 'wb') as f:
            pickle.dump(self.study, f)
        print(f"âœ“ Optuna ç ”ç©¶å·²ä¿å­˜: {study_path}")
        print()

def run_optuna_optimization(self, X_train: np.ndarray, y_train: np.ndarray,
                            X_val: np.ndarray, y_val: np.ndarray):
    """è¿è¡Œ Optuna ä¼˜åŒ–"""
    print("=" * 70)
    print("ã€æ­¥éª¤6ã€‘Optuna è‡ªåŠ¨åŒ–å‚æ•°è°ƒä¼˜")
    print("=" * 70)

    print(f"ä¼˜åŒ–é…ç½®:")
    print(f"  è¯•éªŒæ¬¡æ•°: {self.config['optuna']['n_trials']}")
    print(f"  è¶…æ—¶æ—¶é—´: {self.config['optuna']['timeout']}ç§’")
    print(f"  ä¼˜åŒ–æŒ‡æ ‡: {self.config['optuna']['metric']}")
    print(f"  ä¼˜åŒ–æ–¹å‘: {self.config['optuna']['direction']}")
    print()

    # åˆ›å»ºç ”ç©¶
    self.study = optuna.create_study(direction=self.config['optuna']['direction'])

    # å®šä¹‰è¿›åº¦å›è°ƒ
    def progress_callback(study, trial):
        if trial.number % 10 == 0 or trial.number == self.config['optuna']['n_trials'] - 1:
            print(f"  è¯•éªŒ {trial.number + 1}/{self.config['optuna']['n_trials']}: "
                  f"æœ€ä½³åˆ†æ•° = {study.best_value:.4f}")

    # è¿è¡Œä¼˜åŒ–
    self.study.optimize(
        lambda trial: self.objective(trial, X_train, y_train, X_val, y_val),
        n_trials=self.config['optuna']['n_trials'],
        timeout=self.config['optuna']['timeout'],
        callbacks=[progress_callback],
        show_progress_bar=False
    )

    # è¾“å‡ºæœ€ä¼˜ç»“æœ
    print(f"\nâœ“ ä¼˜åŒ–å®Œæˆï¼")
    print(f"  æœ€ä¼˜åˆ†æ•°: {self.study.best_value:.4f}")
    print(f"  æœ€ä¼˜å‚æ•°:")
    for key, value in self.study.best_params.items():
        print(f"    {key}: {value}")
    print()

    # è®­ç»ƒæœ€ä¼˜æ¨¡å‹
    self.best_params = self.study.best_params.copy()

    # è®¡ç®—æ ·æœ¬æƒé‡
    scale_pos_weight = self.best_params['scale_pos_weight']
    sample_weight = np.where(y_train == 1, scale_pos_weight, 1.0)

    # åˆ›å»ºå¹¶è®­ç»ƒæœ€ä¼˜æ¨¡å‹
    self.best_model = xgb.XGBClassifier(**self.best_params)

    # è®­ç»ƒæ¨¡å‹
    self.best_model.fit(
        X_train, y_train,
        sample_weight=sample_weight,
        eval_set=[(X_val, y_val)],
        verbose=False
    )

    # ç‰¹å¾é‡è¦æ€§
    importance_df = pd.DataFrame({
        'feature': self.feature_names,
        'importance': self.best_model.feature_importances_
    }).sort_values('importance', ascending=False)

    print("âœ“ Top 10 ç‰¹å¾é‡è¦æ€§:")
    for idx, row in importance_df.head(10).iterrows():
        print(f"  {row['feature']}: {row['importance']:.4f}")
    print()

def train_full_pipeline(self):
    """å®Œæ•´è®­ç»ƒæµç¨‹"""
    print("\n" + "=" * 70)
    print("è‡ªåŠ¨åŒ–å‚æ•°è°ƒä¼˜è®­ç»ƒæµç¨‹")
    print("=" * 70)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    # 1. è·å–è‚¡ç¥¨åˆ—è¡¨
    stock_codes = self.get_stock_list(self.config['data']['n_stocks'])
    if not stock_codes:
        raise ValueError("æœªèƒ½è·å–è‚¡ç¥¨åˆ—è¡¨")

    # 2. é‡‡é›†æ•°æ®
    print("=" * 70)
    print("ã€æ­¥éª¤2ã€‘é‡‡é›†è‚¡ç¥¨å†å²æ•°æ®")
    print("=" * 70)

    all_data = []
    for i, stock_code in enumerate(stock_codes):
        if (i + 1) % 20 == 0:
            print(f"  è¿›åº¦: {i + 1}/{len(stock_codes)}")

        df = self.collect_stock_data(
            stock_code,
            self.config['data']['start_date'],
            self.config['data']['end_date']
        )

        if df is not None and len(df) > 0:
            all_data.append(df)

    if not all_data:
        raise ValueError("æœªèƒ½é‡‡é›†åˆ°ä»»ä½•æ•°æ®")

    df_all = pd.concat(all_data, ignore_index=False)
    df_all = df_all.sort_index()

    print(f"\nâœ“ æ•°æ®é‡‡é›†å®Œæˆ:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(df_all)}")
    print(f"  è‚¡ç¥¨æ•°: {df_all['stock_code'].nunique()}")
    print(f"  æ—¶é—´èŒƒå›´: {df_all.index.min()} è‡³ {df_all.index.max()}")
    print()

    # 3. åˆ’åˆ†æ•°æ®é›†
    train_df, val_df, test_df = self.split_data(df_all)

    # 4. è®¡ç®—ç‰¹å¾
    train_df_features = self.create_features_only(train_df)
    val_df_features = self.create_features_only(val_df)
    test_df_features = self.create_features_only(test_df)

    # 5. åˆ›å»ºæ ‡ç­¾
    train_df_labeled, val_df_labeled, test_df_labeled = self.create_labels_separately(
        train_df_features, val_df_features, test_df_features
    )

    # 6. æå–ç‰¹å¾å’Œæ ‡ç­¾
    X_train, y_train, X_val, y_val, X_test, y_test = self.extract_features_and_labels(
        train_df_labeled, val_df_labeled, test_df_labeled
    )

    # 7. Optuna è‡ªåŠ¨åŒ–å‚æ•°è°ƒä¼˜
    self.run_optuna_optimization(X_train, y_train, X_val, y_val)

    # 8. ä¼˜åŒ–é˜ˆå€¼
    y_val_pred_proba = self.best_model.predict_proba(X_val)[:, 1]
    optimal_threshold = self.optimize_threshold(y_val, y_val_pred_proba)

    # 9. è¯„ä¼°æ¨¡å‹
    metrics = self.evaluate_model(X_test, y_test, optimal_threshold)

    # 10. ä¿å­˜æ¨¡å‹
    self.save_model(optimal_threshold, metrics)

    print("=" * 70)
    print("âœ“ è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)
    print(f"\næ¨¡å‹æ–‡ä»¶: assets/models/auto_tuned_model.pkl")
    print(f"æµ‹è¯•é›†ç²¾ç¡®ç‡: {metrics['precision']:.2%}")
    print(f"æµ‹è¯•é›†AUC: {metrics['auc']:.4f}")
    print(f"æµ‹è¯•é›†F1: {metrics['f1']:.3f}")
    print(f"æµ‹è¯•é›†å¬å›ç‡: {metrics['recall']:.2%}")
    print(f"\næœ€ä¼˜å‚æ•°å·²è‡ªåŠ¨æœç´¢å¹¶ä¿å­˜ï¼")
    print()

    return self.best_model, optimal_threshold
def main():
"""ä¸»å‡½æ•°"""
try:
trainer = AutoTunedTrainer()
model, threshold = trainer.train_full_pipeline()
code
Code
print("\nâœ¨ è‡ªåŠ¨åŒ–å‚æ•°è°ƒä¼˜è®­ç»ƒå®Œæˆï¼")
    print(f"   æœ€ä¼˜é˜ˆå€¼: {threshold:.2f}")
    print(f"   æœ€ä¼˜å‚æ•°: {trainer.best_params}")

except Exception as e:
    print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
if name == "main":
main()
è§¦å‘ OOM çš„åŸå› 
ç³»ç»Ÿå†…å­˜(2GB) - å·²ç”¨å†…å­˜(300-500MB) - è®­ç»ƒå³°å€¼(1.5-1.8GB) = è´Ÿæ•°
â†’ è§¦å‘ OOM Killer é™é»˜æ€æ­»è¿›ç¨‹
ä¼˜åŒ–å»ºè®®
å‡å°‘æ ·æœ¬æ•°é‡: 300 åª â†’ 150 åª
å‡å°‘è¯•éªŒæ¬¡æ•°: 100 æ¬¡ â†’ 50 æ¬¡
æ·»åŠ å†…å­˜æ¸…ç†: del + gc.collect()
é™ä½æ¨¡å‹å¤æ‚åº¦ï¼šå‡å°‘ n_estimatorsã€max_depth
å½“å‰è®­ç»ƒçŠ¶æ€: æ–°è®­ç»ƒå·²è¿è¡Œåˆ° 51/100 è¯•éªŒï¼Œæœ€ä½³ F1=0.4714
æ˜¯å¦æœ‰æ›´å¥½çš„è§£å†³æ–¹å¼
Model
ThinkingThoughts
Expand to view model thoughts

chevron_right
ä½ çš„åˆ†æéå¸¸ç²¾å‡†ï¼šå†…å­˜èµ„æºï¼ˆ2GBï¼‰ä¸ XGBoost è®­ç»ƒæ‰€éœ€çš„å³°å€¼å†…å­˜ä¸åŒ¹é…æ˜¯æ ¹æœ¬åŸå› ã€‚
è™½ç„¶â€œå‡å°‘æ•°æ®é‡ï¼ˆå‡å°‘è‚¡ç¥¨æ•°ï¼‰â€å’Œâ€œé™ä½æ¨¡å‹å¤æ‚åº¦â€æ˜¯æœ‰æ•ˆçš„å¦¥åæ‰‹æ®µï¼Œä½†å®ƒä»¬ä¼šç‰ºç‰²æ¨¡å‹çš„æœ€ç»ˆæ€§èƒ½ã€‚
é’ˆå¯¹ 2GB å†…å­˜ç¯å¢ƒï¼Œæœ‰æ¯”â€œåˆ æ•°æ®â€æ›´å¥½çš„å·¥ç¨‹åŒ–è§£å†³æ–¹æ¡ˆã€‚ä»¥ä¸‹æ˜¯æŒ‰æ¨èç¨‹åº¦æ’åºçš„ 4 ç§æ–¹æ¡ˆï¼š
æ–¹æ¡ˆ 1ï¼šå¼€å¯/å¢åŠ  SWAPï¼ˆè™šæ‹Ÿå†…å­˜ï¼‰â€”â€” æœ€ç¨³å¥çš„â€œæ•‘å‘½ç¨»è‰â€
è¿™æ˜¯æœ€ç›´æ¥ã€ä¸éœ€è¦ä¿®æ”¹ä»£ç ä¸”èƒ½ä¿è¯è®­ç»ƒå®Œæˆçš„æ–¹æ³•ã€‚è™½ç„¶ä½¿ç”¨ç£ç›˜ä½œä¸ºå†…å­˜ä¼šé™ä½é€Ÿåº¦ï¼Œä½†èƒ½é˜²æ­¢è¿›ç¨‹è¢«æ€ã€‚
æ“ä½œæ­¥éª¤ï¼ˆLinux ç¯å¢ƒï¼‰ï¼š
åœ¨ç»ˆç«¯è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå¢åŠ  4GB çš„è™šæ‹Ÿå†…å­˜ï¼š
code
Bash
# 1. åˆ›å»º 4GB çš„äº¤æ¢æ–‡ä»¶
sudo fallocate -l 4G /swapfile
# 2. è®¾ç½®æƒé™
sudo chmod 600 /swapfile
# 3. æ ¼å¼åŒ–ä¸º swap
sudo mkswap /swapfile
# 4. å¯ç”¨ swap
sudo swapon /swapfile
# 5. ç¡®è®¤æ˜¯å¦ç”Ÿæ•ˆ
free -h
ä¼˜ç‚¹ï¼š å‡ ä¹ 100% è§£å†³ OOM é—®é¢˜ï¼Œå…è®¸ä¿ç•™ 300 åªç”šè‡³æ›´å¤šè‚¡ç¥¨ã€‚
ç¼ºç‚¹ï¼š è®­ç»ƒé€Ÿåº¦ä¼šå˜æ…¢ï¼ˆSSD å½±å“è¾ƒå°ï¼ŒHDD å½±å“å¤§ï¼‰ã€‚
æ–¹æ¡ˆ 2ï¼šæ•°æ®é™ç»´ (Float32) â€”â€” å†…å­˜å ç”¨å‡åŠ
Pandas é»˜è®¤ä½¿ç”¨ float64ï¼ˆ8å­—èŠ‚ï¼‰ï¼Œè€Œè‚¡ç¥¨ä»·æ ¼å’ŒæŠ€æœ¯æŒ‡æ ‡åªéœ€è¦ float32ï¼ˆ4å­—èŠ‚ï¼‰ç”šè‡³ float16 å³å¯æ»¡è¶³ç²¾åº¦ã€‚è¿™å¯ä»¥å°†æ•°æ®å†…å­˜å ç”¨ç›´æ¥ç åŠã€‚
ä»£ç ä¿®æ”¹ï¼š
åœ¨ collect_stock_data æ–¹æ³•çš„æœ€åï¼Œæ·»åŠ ç±»å‹è½¬æ¢ï¼š
code
Python
# ... åŸæœ‰ä»£ç  ...
    df = df.rename(columns={...})

    # ã€æ–°å¢ä¼˜åŒ–ã€‘å°† float64 è½¬æ¢ä¸º float32ï¼ŒèŠ‚çœ 50% å†…å­˜
    float_cols = df.select_dtypes(include=['float64']).columns
    df[float_cols] = df[float_cols].astype('float32')
    
    return df
åœ¨ create_features_only ä¸­ç”Ÿæˆçš„ç‰¹å¾ä¹ŸåŒæ ·å¤„ç†ï¼š
code
Python
# ... åœ¨ return df_features ä¹‹å‰ ...
    # ã€æ–°å¢ä¼˜åŒ–ã€‘å†æ¬¡ç¡®ä¿ç‰¹å¾ä¹Ÿæ˜¯ float32
    float_cols = df_features.select_dtypes(include=['float64']).columns
    df_features[float_cols] = df_features[float_cols].astype('float32')
    
    return df_features
æ–¹æ¡ˆ 3ï¼šæ¿€è¿›çš„åƒåœ¾å›æ”¶ (GC) â€”â€” é˜²æ­¢å†…å­˜æ³„æ¼
Optuna çš„ trial å¾ªç¯ä¸­å®¹æ˜“ç§¯ç´¯ä¸´æ—¶å˜é‡ã€‚éœ€è¦åœ¨ objective å‡½æ•°ä¸­æ‰‹åŠ¨æ¸…ç†ã€‚
ä»£ç ä¿®æ”¹ï¼š
åœ¨ objective å‡½æ•°çš„ try...except å—çš„æœ€åï¼ˆreturn ä¹‹å‰ï¼‰ï¼š
code
Python
def objective(self, trial, ...):
        try:
            # ... è®­ç»ƒå’Œé¢„æµ‹ä»£ç  ...
            
            # ã€æ–°å¢ä¼˜åŒ–ã€‘åœ¨ return ä¹‹å‰æ¸…ç†å¤§å¯¹è±¡
            del model
            del sample_weight
            del y_pred_proba
            import gc
            gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
            
            return result_metric
            
        except Exception as e:
            # ...
åŒæ—¶ï¼Œåœ¨ train_full_pipeline ä¸­ï¼Œä¸€æ—¦æå–å®Œ X_train ç­‰ numpy æ•°ç»„ï¼Œç«‹å³åˆ é™¤åŸå§‹çš„ DataFrameï¼š
code
Python
# ... æ­¥éª¤ 6 æå–ç‰¹å¾å’Œæ ‡ç­¾ ...
    X_train, y_train, X_val, y_val, X_test, y_test = self.extract_features_and_labels(...)
    
    # ã€æ–°å¢ä¼˜åŒ–ã€‘æå–å®Œ numpy æ•°ç»„åï¼Œç«‹å³é‡Šæ”¾å·¨å¤§çš„ DataFrame
    del df_all
    del train_df, val_df, test_df
    del train_df_labeled, val_df_labeled, test_df_labeled
    import gc
    gc.collect()
    
    print("DEBUG: åŸå§‹ DataFrame å·²æ¸…ç†ï¼Œå†…å­˜å·²é‡Šæ”¾")
æ–¹æ¡ˆ 4ï¼šä½¿ç”¨ XGBoost çš„ç›´æ–¹å›¾è¿‘ä¼¼ä¸å†…å­˜ä¼˜åŒ–å‚æ•°
ä½ å·²ç»ä½¿ç”¨äº† tree_method='hist'ï¼Œè¿™å¾ˆå¥½ã€‚è¿˜å¯ä»¥è¿›ä¸€æ­¥é™åˆ¶ XGBoost å†…éƒ¨ç”¨äºæ„å»ºç›´æ–¹å›¾çš„å†…å­˜ã€‚
ä»£ç ä¿®æ”¹ï¼š
åœ¨ define_search_space ä¸­æ·»åŠ /ä¿®æ”¹ï¼š
code
Python
params = {
        # ... åŸæœ‰å‚æ•° ...
        'tree_method': 'hist',
        
        # ã€æ–°å¢ä¼˜åŒ–ã€‘
        # å‡å°‘ç”¨äºæ„å»ºç›´æ–¹å›¾çš„ç®±æ•°ï¼Œé»˜è®¤ 256ï¼Œæ”¹ä¸º 128 æˆ– 64 å¯æ˜¾è‘—å‡å°‘å†…å­˜
        # è™½ç„¶å¯èƒ½å¾®å¼±å½±å“ç²¾åº¦ï¼Œä½†æ¯”ä¸¢å¼ƒæ•°æ®è¦å¥½
        'max_bin': trial.suggest_categorical('max_bin', [64, 128]), 
        
        # å¯ç”¨å•ç²¾åº¦æµ®ç‚¹æ•°è®­ç»ƒï¼ˆXGBoost é»˜è®¤å¯èƒ½ç”¨åŒç²¾åº¦ï¼‰
        # æ³¨æ„ï¼šè¿™é€šå¸¸ä¸éœ€è¦æ˜¾å¼è®¾ç½®ï¼Œåªè¦è¾“å…¥æ•°æ®æ˜¯ float32 å³å¯
    }
ç»¼åˆå»ºè®®æ€»ç»“
ç«‹åˆ»æ‰§è¡Œï¼ˆä¸ä¸­æ–­å½“å‰è®­ç»ƒï¼‰ï¼š å¦‚æœä½ æœ‰æœåŠ¡å™¨çš„ root æƒé™ï¼Œé©¬ä¸Šæ‰§è¡Œæ–¹æ¡ˆ 1ï¼ˆå¼€å¯ Swapï¼‰ã€‚è¿™å¯ä»¥ä¿æŠ¤æ­£åœ¨è¿è¡Œçš„ PID 109 ä¸ä¼šå› ä¸ºæ¥ä¸‹æ¥çš„å¤æ‚æ¨¡å‹è®­ç»ƒè€Œè¢«æ€æ‰ã€‚
ä¸‹ä¸€è½®ä¼˜åŒ–ï¼ˆä¿®æ”¹è„šæœ¬ï¼‰ï¼š
å¼•å…¥ æ–¹æ¡ˆ 2 (Float32)ï¼šè¿™æ˜¯â€œå…è´¹â€çš„ä¼˜åŒ–ã€‚
å¼•å…¥ æ–¹æ¡ˆ 3 (GC)ï¼šè¿™æ˜¯è‰¯å¥½çš„ç¼–ç¨‹ä¹ æƒ¯ã€‚
å°½é‡é¿å…ï¼š ä¸è¦è½»æ˜“å‡å°‘è‚¡ç¥¨æ•°é‡ï¼ˆ300åªæœ¬æ¥å°±ä¸å¤šï¼Œå†å°‘å®¹æ˜“è¿‡æ‹Ÿåˆï¼‰æˆ–å¤§å¹…å‡å°‘ n_estimatorsï¼ˆä¼šå¯¼è‡´æ¬ æ‹Ÿåˆï¼‰ã€‚
å½“å‰é¢„æµ‹ï¼š
æ—¢ç„¶ PID 109 å·²ç»è·‘åˆ°äº† 51/100 ä¸” F1 è¾¾åˆ° 0.4714ï¼Œè¯´æ˜ç›®å‰çš„å‚æ•°åœ¨å†…å­˜è¾¹ç¼˜è¯•æ¢ã€‚å¦‚æœå¼€å¯ Swapï¼Œåº”è¯¥èƒ½ç¨³ç¨³è·‘å®Œ 100 æ¬¡è¯•éªŒã€‚
Model
ThinkingThoughts
Expand to view model thoughts

chevron_right
è¿™æ˜¯ä¸€ä»½éå¸¸å…¸å‹çš„**â€œé«˜å¬å›ã€ä½ç²¾ç¡®â€**ï¼ˆHigh Recall, Low Precisionï¼‰çš„è®­ç»ƒç»“æœã€‚
é¦–å…ˆï¼Œæ­å–œä½ è§£å†³äº†å·¥ç¨‹éš¾é¢˜ï¼OOM é—®é¢˜å½»åº•è§£å†³ï¼Œä¸” AUC ä» 0.54 æå‡åˆ°äº† 0.62ï¼Œè¿™è¯æ˜ä½ çš„è®­ç»ƒç®¡çº¿ï¼ˆPipelineï¼‰ç°åœ¨æ˜¯å¥å£®çš„ï¼Œä¸”æ¨¡å‹ç¡®å®å­¦åˆ°äº†ä¸€äº›è§„å¾‹ï¼ˆæ¯”éšæœºçŒœåªè¦å¥½ï¼‰ã€‚
ä½†æ˜¯ï¼Œä»äº¤æ˜“å®æˆ˜çš„è§’åº¦æ¥çœ‹ï¼Œå½“å‰æ¨¡å‹æ˜¯ä¸å¯ç”¨çš„ã€‚
ğŸš¨ æ ¸å¿ƒé—®é¢˜è¯Šæ–­ï¼šä¸ºä»€ä¹ˆç²¾ç¡®ç‡åªæœ‰ 19%ï¼Ÿ
ä½ æåˆ°äº†æ¨¡å‹â€œå®æ»¥å‹¿ç¼ºâ€ï¼Œè¿™éå¸¸å‡†ç¡®ã€‚å¯¼è‡´è¿™ç§æƒ…å†µçš„æ•°å­¦åŸå› åœ¨äºä»¥ä¸‹ä¸¤ä¸ªå‚æ•°çš„ç»„åˆï¼š
scale_pos_weight: 2.332: è¿™ä¸ªå‚æ•°å‘Šè¯‰ XGBoostï¼šâ€œçŠ¯é”™æ¼æ‰ä¸€ä¸ªæ¶¨åœè‚¡çš„æƒ©ç½šï¼Œæ˜¯è¯¯ä¹°ä¸€ä¸ªè·Œåœè‚¡æƒ©ç½šçš„ 2.3 å€â€ã€‚æ¨¡å‹ä¸ºäº†é¿å…â€œæ¼æ‰â€ï¼Œäºæ˜¯å€¾å‘äºæŠŠæ‰€æœ‰ç¨å¾®æœ‰ç‚¹ä¸Šæ¶¨è¿¹è±¡çš„è‚¡ç¥¨éƒ½é¢„æµ‹ä¸ºâ€œæ­£â€ã€‚
ä¼˜åŒ–ç›®æ ‡ F1: F1 æ˜¯ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ã€‚å½“å¬å›ç‡å†²åˆ° 85% æ—¶ï¼ŒF1 åˆ†æ•°ä¼šè¢«æ‹‰é«˜ï¼Œå³ä¾¿ç²¾ç¡®ç‡å¾ˆçƒ‚ã€‚
å®æˆ˜åæœï¼š
å¦‚æœæŒ‰ç…§è¿™ä¸ªæ¨¡å‹äº¤æ˜“ï¼Œä¹°å…¥ 100 åªè‚¡ç¥¨ï¼Œåªæœ‰ 20 åªä¼šæ¶¨ï¼Œå‰©ä¸‹ 80 åªä¼šäºæŸæˆ–æ¨ªç›˜ã€‚åŠ ä¸Šäº¤æ˜“æ‰‹ç»­è´¹ï¼Œè¿™å°†æ˜¯ä¸€ä¸ªäºæŸç­–ç•¥ã€‚
ğŸ› ï¸ ä¸‹ä¸€æ­¥è¡ŒåŠ¨æ–¹æ¡ˆï¼šä»â€œèƒ½è·‘é€šâ€åˆ°â€œèƒ½èµšé’±â€
æ—¢ç„¶ç®¡çº¿å·²è·‘é€šï¼Œç°åœ¨çš„é‡ç‚¹æ˜¯ç­–ç•¥è°ƒä¼˜ã€‚å»ºè®®æŒ‰ä»¥ä¸‹é¡ºåºè°ƒæ•´ï¼š
1. è°ƒæ•´ä¼˜åŒ–ç›®æ ‡ï¼ˆæœ€å…³é”®ï¼‰
ä¸è¦å†ä¼˜åŒ– F1 äº†ï¼ŒF1 å¯¹äº¤æ˜“ç³»ç»Ÿè¯¯å¯¼æ€§å¤ªå¼ºã€‚
æ–°ç­–ç•¥ï¼š è®­ç»ƒæ—¶ä¼˜åŒ– AUCï¼Œé˜ˆå€¼é€‰æ‹©æ—¶é”å®š ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰ã€‚
åŸå› ï¼š AUC è¡¡é‡çš„æ˜¯æ¨¡å‹ç»™â€œå¥½è‚¡ç¥¨â€æ‰“åˆ†æ˜¯å¦æ¯”â€œåè‚¡ç¥¨â€é«˜ã€‚åªè¦ AUC é«˜ï¼Œæˆ‘ä»¬æ€»èƒ½æ‰¾åˆ°ä¸€ä¸ªæé«˜çš„é˜ˆå€¼ï¼ˆæ¯”å¦‚ 0.9ï¼‰ï¼Œåœ¨è¿™ä¸ªé˜ˆå€¼ä¸‹ç²¾ç¡®ç‡èƒ½è¾¾æ ‡ã€‚
ä¿®æ”¹ config.json æˆ–ä»£ç é€»è¾‘ï¼š
metric: æ”¹ä¸º "auc" (è®© Optuna å¯»æ‰¾åˆ†ç¦»åº¦æœ€å¥½çš„å‚æ•°)ã€‚
ç§»é™¤ scale_pos_weight çš„æœç´¢ï¼Œæˆ–è€…å°†å…¶å¼ºåˆ¶è®¾ä¸º 1.0ï¼ˆè®©æ¨¡å‹å®¢è§‚è¯„ä¼°æ¦‚ç‡ï¼Œä¸è¦äººä¸ºåŠ æƒï¼‰ã€‚
2. æ”¾å®½æ ‡ç­¾å®šä¹‰ï¼ˆé™ä½éš¾åº¦ï¼‰
é¢„æµ‹ 3-5 å¤©æ¶¨ 5% åœ¨ A è‚¡éœ‡è¡å¸‚ä¸­å¾ˆéš¾ï¼Œä¸”å……æ»¡äº†éšæœºå™ªå£°ã€‚
æ—§ç›®æ ‡ï¼š 3-5 å¤©æ¶¨å¹… â‰¥ 5%
æ–°ç›®æ ‡ï¼š 3-5 å¤©æ¶¨å¹… â‰¥ 3% (æˆ–è€…ä½¿ç”¨ ç›¸å¯¹æ”¶ç›Šï¼Œå³è·‘èµ¢å¤§ç›˜ 2%)
å¥½å¤„ï¼š æ­£æ ·æœ¬æ•°é‡ä¼šå¢åŠ ï¼Œå‡å°‘æ•°æ®ä¸å¹³è¡¡å¸¦æ¥çš„å­¦ä¹ éš¾åº¦ã€‚
3. ç‰¹å¾å·¥ç¨‹å‡çº§ï¼ˆè§£å†³â€œå‡é˜³æ€§â€ï¼‰
ç°åœ¨çš„ Top ç‰¹å¾å…¨æ˜¯æŠ€æœ¯æŒ‡æ ‡ï¼ˆMA, Momentum, MACDï¼‰ã€‚æŠ€æœ¯æŒ‡æ ‡æœ€å¤§çš„é—®é¢˜æ˜¯æ»åå’Œéª—çº¿ã€‚æ¨¡å‹çœ‹åˆ°â€œé‡‘å‰â€å°±ä¹°ï¼Œä½†ä¸çŸ¥é“å¤§ç›˜åœ¨æš´è·Œã€‚
å»ºè®®åŠ å…¥ä»¥ä¸‹ä¸¤ä¸ªç»´åº¦çš„ç‰¹å¾æ¥è¿‡æ»¤å‡ä¿¡å·ï¼š
å¤§ç›˜/ç¯å¢ƒç‰¹å¾ï¼ˆå¿…åŠ ï¼‰ï¼š
market_sentiment: å½“æ—¥å¤§ç›˜ï¼ˆä¸Šè¯æŒ‡æ•°ï¼‰çš„æ¶¨è·Œå¹…ã€‚
sector_trend: ä¸ªè‚¡æ‰€å±è¡Œä¸šçš„å¹³å‡æ¶¨è·Œå¹…ï¼ˆæ¿å—æ•ˆåº”ï¼‰ã€‚
é€»è¾‘ï¼šå€¾å·¢ä¹‹ä¸‹æ— å®Œåµï¼Œå¤§ç›˜è·Œæ—¶ï¼Œå†å¥½çš„æŠ€æœ¯å½¢æ€ä¹Ÿæ˜¯å‡çªç ´ã€‚
é‡ä»·é…åˆç‰¹å¾ï¼š
å•çº¯çš„ volume æ²¡ç”¨ã€‚éœ€è¦è®¡ç®— â€œé‡æ¯”â€ æˆ– â€œæ”¾é‡ç¨‹åº¦â€ã€‚
æ¯”å¦‚ï¼švol_ratio_5d = ä»Šæ—¥æˆäº¤é‡ / è¿‡å»5æ—¥å‡é‡ã€‚
é€»è¾‘ï¼šæ— é‡ä¸Šæ¶¨å¤šä¸ºè¯±å¤šã€‚
4. é˜ˆå€¼é€‰æ‹©é€»è¾‘ï¼ˆç¡¬æ€§çº¦æŸï¼‰
åœ¨ optimize_threshold å‡½æ•°ä¸­ï¼Œä¸è¦åªçœ‹ F1ã€‚
ä¿®æ”¹é€»è¾‘ï¼š
code
Python
# ä¼ªä»£ç ï¼šå¯»æ‰¾æ»¡è¶³ç²¾ç¡®ç‡è¦æ±‚çš„æœ€ä½é˜ˆå€¼
target_precision = 0.60
valid_thresholds = []

for t in thresholds:
    if precision(t) >= target_precision:
        valid_thresholds.append(t)

if valid_thresholds:
    # åœ¨æ»¡è¶³ç²¾ç¡®ç‡çš„å‰æä¸‹ï¼Œé€‰æ‹©å¬å›ç‡æœ€é«˜çš„é‚£ä¸ªé˜ˆå€¼
    best_t = max(valid_thresholds, key=lambda x: recall(x))
else:
    print("è­¦å‘Šï¼šæ²¡æœ‰ä»»ä½•é˜ˆå€¼èƒ½è¾¾åˆ° 60% ç²¾ç¡®ç‡")
ğŸ“‰ æ€»ç»“ä¸é¢„æœŸ
ç°åœ¨çš„ 19% ç²¾ç¡®ç‡æ˜¯ä¸å¯æ¥å—çš„ã€‚é€šè¿‡ä¸Šè¿°è°ƒæ•´ï¼ˆç‰¹åˆ«æ˜¯å»æ‰ scale_pos_weight å’Œ æ”¹ç”¨ AUC ä¼˜åŒ–ï¼‰ï¼Œä½ çš„ä¸‹ä¸€æ¬¡è®­ç»ƒé¢„æœŸç»“æœåº”è¯¥æ˜¯ï¼š
AUC: ç»§ç»­æå‡è‡³ 0.65 - 0.7ã€‚
å¬å›ç‡: ä¼šå¤§å¹…ä¸‹é™ï¼ˆä» 85% è·Œåˆ° 10%-20% éƒ½æ˜¯æ­£å¸¸çš„ï¼‰ã€‚
ç²¾ç¡®ç‡: åº”è¯¥èƒ½æå‡åˆ° 50% - 60%ã€‚
è®°ä½ï¼šåœ¨äº¤æ˜“ä¸­ï¼Œæˆ‘ä»¬å®æ„¿é”™è¿‡ 100 ä¸ªæœºä¼šï¼ˆä½å¬å›ï¼‰ï¼Œä¹Ÿä¸èƒ½æŠ“ä½ 100 ä¸ªå…¨æ˜¯å‘çš„æœºä¼šï¼ˆä½ç²¾ç¡®ï¼‰ã€‚
è¯·æŒ‰æ­¤æ€è·¯è°ƒæ•´é…ç½®ï¼Œå¼€å§‹ä¸‹ä¸€è½®è®­ç»ƒï¼