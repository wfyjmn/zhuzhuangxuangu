# -*- coding: utf-8 -*-
"""
ä½¿ç”¨çœŸå®å†å²æ•°æ®è®­ç»ƒ AI è£åˆ¤æ¨¡å‹ï¼ˆç»ˆæä¼˜åŒ–ç‰ˆï¼‰
ä¼˜åŒ–ç‚¹ï¼š
1. é›†æˆ DataWarehouseTurbo å®ç°æé€Ÿæ•°æ®ç”Ÿæˆ
2. è‡ªåŠ¨å¯¼å‡ºç‰¹å¾é‡è¦æ€§ (Feature Importance)
3. å¢å¼ºå†…å­˜ç®¡ç†ä¸åƒåœ¾å›æ”¶
4. ä¿®æ­£ CSV è¯»å–æ—¶çš„æ—¥æœŸæ ¼å¼é—®é¢˜
"""
import os
import sys
import logging
import gc
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

# å°è¯•å¯¼å…¥æ ¸å¿ƒæ¨¡å—
try:
    from ai_backtest_generator import AIBacktestGenerator
    from ai_referee import AIReferee

    # [ä¼˜åŒ–] å°è¯•å¯¼å…¥ Turbo ç‰ˆæœ¬
    try:
        from data_warehouse_turbo import DataWarehouse
        IS_TURBO = True
    except ImportError:
        from data_warehouse import DataWarehouse
        IS_TURBO = False
except ImportError as e:
    print(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)

# é…ç½®æ—¥å¿—
log_dir = project_root / 'logs'
log_dir.mkdir(exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(log_dir / 'train_optimized.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ========================================
# é…ç½®å‚æ•°ï¼ˆå¯ä¿®æ”¹ï¼‰
# ========================================
TRAINING_CONFIG = {
    # æ—¶é—´èŒƒå›´ (2023-2024 å¹´å®Œæ•´æ•°æ®)
    'start_date': '20230101',
    'end_date': '20241231',

    # æ•°æ®ç”Ÿæˆå‚æ•°
    'amount_threshold': 10000,  # æˆäº¤é¢é˜ˆå€¼ï¼ˆåƒå…ƒï¼‰
    'max_candidates': 100,      # æ¯æ—¥æœ€å¤§å€™é€‰è‚¡ç¥¨æ•°ï¼ˆå¢åŠ ä»¥è·å¾—æ›´å¤šæ ·æœ¬ï¼‰
    'max_samples': 500000,      # ã€æ‰‹æœ¯ä¸€ã€‘å½»åº•æ”¾å¼€æ ·æœ¬é™åˆ¶ï¼ˆ50ä¸‡ï¼‰ï¼Œè·‘å®Œ 2023-2024 å…¨å¹´

    # è®­ç»ƒå‚æ•°
    'n_splits': 5,              # äº¤å‰éªŒè¯æŠ˜æ•°
    'model_type': 'xgboost',    # æ¨¡å‹ç±»å‹

    # å†…å­˜ä¼˜åŒ–
    'use_float32': True,        # ä½¿ç”¨ float32 èŠ‚çœå†…å­˜
}


def generate_training_data(config: dict):
    """
    ç”Ÿæˆè®­ç»ƒæ•°æ®é›†
    """
    logger.info("=" * 80)
    logger.info("ã€æ­¥éª¤ 1ã€‘ç”Ÿæˆè®­ç»ƒæ•°æ®é›†")
    logger.info("=" * 80)

    # åˆå§‹åŒ–æ•°æ®ä»“åº“
    dw = DataWarehouse()
    generator = AIBacktestGenerator()

    # [ä¼˜åŒ–] Turbo æ¨¡å¼é¢„åŠ è½½
    if IS_TURBO and hasattr(dw, 'preload_data'):
        logger.info("[ç³»ç»Ÿ] å¯åŠ¨ Turbo æé€Ÿæ¨¡å¼ï¼šé¢„åŠ è½½æ•°æ®åˆ°å†…å­˜")
        # æ‰©å±•ç»“æŸæ—¥æœŸä»¥åŒ…å«æ ‡ç­¾æ‰€éœ€çš„æœªæ¥æ•°æ® (Labeling éœ€è¦æœªæ¥5-10å¤©æ•°æ®)
        dt_end = datetime.strptime(config['end_date'], '%Y%m%d')
        extended_end = (dt_end + timedelta(days=20)).strftime('%Y%m%d')

        dw.preload_data(config['start_date'], extended_end, lookback_days=120)

        # æ³¨å…¥ Turbo Warehouse
        generator.warehouse = dw
    else:
        logger.warning("[ç³»ç»Ÿ] ä½¿ç”¨æ™®é€šæ¨¡å¼ï¼ˆæ— å†…å­˜é¢„åŠ è½½ï¼‰ï¼Œé€Ÿåº¦è¾ƒæ…¢")

    # åº”ç”¨é…ç½®
    generator.amount_threshold = config['amount_threshold']
    generator.max_candidates = config['max_candidates']

    logger.info(f"\n[é…ç½®]")
    logger.info(f"  æ—¶é—´èŒƒå›´ï¼š{config['start_date']} ~ {config['end_date']}")
    logger.info(f"  æˆäº¤é¢é˜ˆå€¼ï¼š{config['amount_threshold']} åƒå…ƒ")
    logger.info(f"  æœ€å¤§å€™é€‰ï¼š{config['max_candidates']} åª/å¤©")

    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    logger.info("\n[å¼€å§‹] ç”Ÿæˆè®­ç»ƒæ•°æ®...")

    try:
        dataset = generator.generate_dataset(
            start_date=config['start_date'],
            end_date=config['end_date'],
            max_samples=config['max_samples']
        )

        if dataset is None or len(dataset) == 0:
            logger.error("\n[é”™è¯¯] ç”Ÿæˆçš„è®­ç»ƒæ•°æ®ä¸ºç©º")
            return None

        # ç»Ÿè®¡ä¿¡æ¯
        pos_samples = (dataset['label'] == 1).sum()
        neg_samples = (dataset['label'] == 0).sum()
        total_samples = len(dataset)

        logger.info(f"\n[æˆåŠŸ] ç”Ÿæˆè®­ç»ƒæ•°æ®")
        logger.info(f"  æ ·æœ¬æ•°ï¼š{total_samples} æ¡")
        logger.info(f"  æ­£æ ·æœ¬ï¼š{pos_samples} ({pos_samples/total_samples*100:.2f}%)")
        logger.info(f"  è´Ÿæ ·æœ¬ï¼š{neg_samples} ({neg_samples/total_samples*100:.2f}%)")

        # [ä¼˜åŒ–] ä½¿ç”¨ float32 èŠ‚çœå†…å­˜
        if config['use_float32']:
            logger.info("[ä¼˜åŒ–] è½¬æ¢ä¸º float32 æ ¼å¼...")
            numeric_cols = dataset.select_dtypes(include=[np.float64]).columns
            dataset[numeric_cols] = dataset[numeric_cols].astype(np.float32)

        # ä¿å­˜è®­ç»ƒæ•°æ®
        output_dir = project_root / 'data' / 'training'
        output_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        data_file = output_dir / f'training_data_{timestamp}.csv'

        dataset.to_csv(data_file, index=False, encoding='utf-8')
        logger.info(f"\n[ä¿å­˜] è®­ç»ƒæ•°æ®å·²ä¿å­˜ï¼š{data_file}")
        logger.info(f"       æ–‡ä»¶å¤§å°ï¼š{data_file.stat().st_size / 1024 / 1024:.2f} MB")

        # ä¸»åŠ¨é‡Šæ”¾å†…å­˜
        del dataset
        if IS_TURBO:
            dw.clear_memory()  # å¦‚æœæ˜¯Turboï¼Œé‡Šæ”¾å¤§å†…å­˜å—
        gc.collect()

        return str(data_file)

    except Exception as e:
        logger.error(f"\n[é”™è¯¯] ç”Ÿæˆè®­ç»ƒæ•°æ®å¤±è´¥ï¼š{str(e)}", exc_info=True)
        return None


def train_model(data_file: str, config: dict):
    """
    è®­ç»ƒ AI è£åˆ¤æ¨¡å‹
    """
    logger.info("\n" + "=" * 80)
    logger.info("ã€æ­¥éª¤ 2ã€‘è®­ç»ƒ AI è£åˆ¤æ¨¡å‹")
    logger.info("=" * 80)

    try:
        referee = AIReferee(model_type=config['model_type'])

        logger.info(f"\n[è¯»å–] è®­ç»ƒæ•°æ®ï¼š{data_file}")

        # [ä¼˜åŒ–] æŒ‡å®šæ•°æ®ç±»å‹è¯»å–ï¼Œé˜²æ­¢ CSV å°†æ—¥æœŸè¯»æˆæ•´æ•°
        dtype_dict = {'label': np.int32, 'trade_date': str, 'ts_code': str}
        if config['use_float32']:
            # è¿™é‡Œçš„é€»è¾‘ç¨å¾®å¤æ‚ï¼Œæ— æ³•é¢„çŸ¥æ‰€æœ‰åˆ—åï¼Œæ‰€ä»¥åªæŒ‡å®šå…³é”®åˆ—
            pass

        dataset = pd.read_csv(data_file, dtype=dtype_dict)

        # [ä¼˜åŒ–] å†æ¬¡å¼ºåˆ¶è½¬æ¢ float32 (Pandas read_csv é»˜è®¤æ˜¯ float64)
        if config['use_float32']:
            float_cols = dataset.select_dtypes(include=['float64']).columns
            dataset[float_cols] = dataset[float_cols].astype('float32')

        # [å…³é”®] ç¡®ä¿ trade_date æ˜¯å­—ç¬¦ä¸²æˆ– datetimeï¼Œä»¥ä¾¿ TimeSeriesSplit æ­£ç¡®æ’åº
        dataset['trade_date'] = dataset['trade_date'].astype(str)

        logger.info(f"[ä¿¡æ¯] åŸå§‹æ•°æ®å½¢çŠ¶ï¼š{dataset.shape}")
        logger.info(f"[ä¿¡æ¯] å†…å­˜å ç”¨ï¼š{dataset.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")

        # å‡†å¤‡æ•°æ®
        # AIReferee.train_time_series éœ€è¦ trade_date åˆ—è¿›è¡Œæ’åºå’Œåˆ‡åˆ†
        # prepare_features å†…éƒ¨ä¼šè‡ªåŠ¨å¤„ç†å®ƒï¼Œæ‰€ä»¥è¿™é‡Œä¼ å…¥åŒ…å« trade_date çš„ X
        X = dataset.drop('label', axis=1)
        y = dataset['label'].astype(np.int32)

        logger.info(f"[ä¿¡æ¯] æ ·æœ¬æ•°ï¼š{X.shape[0]}")
        logger.info(f"[ä¿¡æ¯] æ­£æ ·æœ¬å æ¯”ï¼š{y.sum()/len(y)*100:.2f}%")

        # æ ·æœ¬ä¸å¹³è¡¡è­¦å‘Š
        if y.sum() / len(y) < 0.05:
            logger.warning(f"[è­¦å‘Š] æ­£æ ·æœ¬æå°‘ï¼Œæ¨¡å‹å¯èƒ½å€¾å‘äºé¢„æµ‹å…¨è´Ÿï¼")

        # è®­ç»ƒæ¨¡å‹
        logger.info(f"\n[å¼€å§‹] è®­ç»ƒæ¨¡å‹ï¼ˆ{config['n_splits']}æŠ˜æ—¶åºäº¤å‰éªŒè¯ï¼‰...")
        logger.info("[æç¤º] è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´")

        results = referee.train_time_series(X, y, n_splits=config['n_splits'])

        logger.info("\n[æˆåŠŸ] æ¨¡å‹è®­ç»ƒå®Œæˆ")

        # æ‰“å°äº¤å‰éªŒè¯ç»“æœ
        logger.info("\n[äº¤å‰éªŒè¯ç»“æœ]")
        if 'cv_results' in results:
            logger.info("\n" + results['cv_results'].to_string(index=False))

        logger.info("\n[å¹³å‡æŒ‡æ ‡]")
        for metric, value in results.get('avg_metrics', {}).items():
            logger.info(f"  {metric}: {value:.4f}")

        # ä¿å­˜æ¨¡å‹
        output_dir = project_root / 'data' / 'models'
        output_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_file = output_dir / f'ai_referee_{config["model_type"]}_{timestamp}.pkl'

        referee.save_model(str(model_file))
        logger.info(f"\n[ä¿å­˜] æ¨¡å‹å·²ä¿å­˜ï¼š{model_file}")
        logger.info(f"       æ–‡ä»¶å¤§å°ï¼š{Path(model_file).stat().st_size / 1024 / 1024:.2f} MB")

        # [æ–°å¢] ä¿å­˜ç‰¹å¾é‡è¦æ€§
        # è¿™å¯¹äºç†è§£æ¨¡å‹é€»è¾‘è‡³å…³é‡è¦
        if hasattr(referee, 'get_feature_importance'):
            imp_df = referee.get_feature_importance()
            if not imp_df.empty:
                imp_file = output_dir / f'feature_importance_{timestamp}.csv'
                imp_df.to_csv(imp_file, index=False)
                logger.info(f"[ä¿å­˜] ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜ï¼š{imp_file}")

                logger.info("\n[Top 10 é‡è¦ç‰¹å¾]")
                for idx, row in imp_df.head(10).iterrows():
                    logger.info(f"  {idx+1}. {row['feature']}: {row['importance']:.4f}")
        else:
            # å¦‚æœ AIReferee æ²¡æœ‰ get_feature_importance æ–¹æ³•ï¼Œæ‰‹åŠ¨æå–
            if hasattr(referee, 'model') and hasattr(referee.model, 'feature_importances_'):
                imps = referee.model.feature_importances_
                if hasattr(referee, 'feature_names'):
                    feature_names = referee.feature_names
                    if len(imps) == len(feature_names):
                        importances = pd.DataFrame({
                            'feature': feature_names,
                            'importance': imps
                        }).sort_values('importance', ascending=False)

                        imp_file = output_dir / f'feature_importance_{timestamp}.csv'
                        importances.to_csv(imp_file, index=False)
                        logger.info(f"[ä¿å­˜] ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜ï¼š{imp_file}")

                        logger.info("\n[Top 10 é‡è¦ç‰¹å¾]")
                        for idx, row in importances.head(10).iterrows():
                            logger.info(f"  {idx+1}. {row['feature']}: {row['importance']:.4f}")

        return True

    except Exception as e:
        logger.error(f"\n[é”™è¯¯] è®­ç»ƒæ¨¡å‹å¤±è´¥ï¼š{str(e)}", exc_info=True)
        return False


def main():
    """ä¸»æµç¨‹"""
    print("=" * 80)
    print("         AI è£åˆ¤ V5.0 è®­ç»ƒæµç¨‹ï¼ˆTurbo å¢å¼ºç‰ˆï¼‰")
    print("=" * 80)

    # æ‰“å°å½“å‰ä½¿ç”¨çš„ä»“åº“æ¨¡å¼
    mode = "ğŸš€ Turbo æé€Ÿæ¨¡å¼" if IS_TURBO else "ğŸ¢ æ™®é€šç¡¬ç›˜æ¨¡å¼"
    print(f"å½“å‰è¿è¡Œæ¨¡å¼: {mode}")

    # æ­¥éª¤ 1ï¼šç”Ÿæˆ
    data_file = generate_training_data(TRAINING_CONFIG)
    if not data_file: return

    # æ­¥éª¤ 2ï¼šè®­ç»ƒ
    success = train_model(data_file, TRAINING_CONFIG)
    if not success: return

    print("\n" + "=" * 80)
    print("âœ… è®­ç»ƒå…¨æµç¨‹å®Œæˆï¼")
    print("=" * 80)


if __name__ == '__main__':
    main()
