"""
è¯¯å·®æº¯æºæ¨¡å—
åŠŸèƒ½ï¼šåˆ†æé¢„æµ‹è¯¯å·®æ¥æºï¼Œå®šä½é—®é¢˜
"""
import os
import json
import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Optional
from collections import Counter
import xgboost as xgb

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ErrorTracker:
    """è¯¯å·®è¿½è¸ªå™¨"""
    
    def __init__(self, predictor=None, config_path: str = None):
        """
        åˆå§‹åŒ–è¯¯å·®è¿½è¸ªå™¨
        
        Args:
            predictor: è‚¡ç¥¨é¢„æµ‹å™¨å®ä¾‹
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        if config_path is None:
            workspace_path = os.getenv("COZE_WORKSPACE_PATH", "/workspace/projects")
            config_path = os.path.join(workspace_path, "config/model_config.json")
        
        self.config = self._load_config(config_path)
        self.predictor = predictor
        self.features = self.config['data']['train_features']
        
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"åŠ è½½é…ç½®æˆåŠŸ")
            return config
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
            return {}
    
    def analyze_errors(self, aligned_df: pd.DataFrame) -> Dict:
        """
        åˆ†æè¯¯å·®åˆ†å¸ƒ
        
        Args:
            aligned_df: å¯¹é½åçš„é¢„æµ‹æ•°æ®
            
        Returns:
            è¯¯å·®åˆ†æç»“æœ
        """
        try:
            if aligned_df.empty:
                logger.warning("å¯¹é½æ•°æ®ä¸ºç©ºï¼Œæ— æ³•åˆ†æè¯¯å·®")
                return {}
            
            error_analysis = {}
            
            # 1. æ€»ä½“è¯¯å·®ç»Ÿè®¡
            total = len(aligned_df)
            errors = aligned_df[aligned_df['predict_correct'] == 0]
            correct = aligned_df[aligned_df['predict_correct'] == 1]
            
            error_analysis['total_samples'] = total
            error_analysis['error_count'] = len(errors)
            error_analysis['correct_count'] = len(correct)
            error_analysis['error_rate'] = len(errors) / total if total > 0 else 0
            
            # 2. å‡æ­£ä¾‹åˆ†æï¼ˆé¢„æµ‹ä¸ºä¸Šæ¶¨ï¼Œå®é™…ä¸ºä¸‹è·Œï¼‰
            false_positives = aligned_df[
                (aligned_df['predicted_label'] == 1) & 
                (aligned_df['actual_label'] == 0)
            ]
            error_analysis['false_positive_count'] = len(false_positives)
            error_analysis['false_positive_rate'] = len(false_positives) / total if total > 0 else 0
            
            # 3. å‡è´Ÿä¾‹åˆ†æï¼ˆé¢„æµ‹ä¸ºä¸‹è·Œï¼Œå®é™…ä¸ºä¸Šæ¶¨ï¼‰
            false_negatives = aligned_df[
                (aligned_df['predicted_label'] == 0) & 
                (aligned_df['actual_label'] == 1)
            ]
            error_analysis['false_negative_count'] = len(false_negatives)
            error_analysis['false_negative_rate'] = len(false_negatives) / total if total > 0 else 0
            
            # 4. æ¦‚ç‡åˆ†å¸ƒåˆ†æ
            error_analysis['probability_distribution'] = self._analyze_probability_distribution(aligned_df)
            
            # 5. æ¶¨è·Œå¹…åˆ†æ
            error_analysis['price_change_analysis'] = self._analyze_price_change_distribution(aligned_df)
            
            logger.info(f"è¯¯å·®åˆ†æå®Œæˆ")
            return error_analysis
            
        except Exception as e:
            logger.error(f"åˆ†æè¯¯å·®å¤±è´¥: {e}")
            return {}
    
    def _analyze_probability_distribution(self, aligned_df: pd.DataFrame) -> Dict:
        """
        åˆ†æé¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        
        Args:
            aligned_df: å¯¹é½æ•°æ®
            
        Returns:
            æ¦‚ç‡åˆ†å¸ƒç»Ÿè®¡
        """
        try:
            # æŒ‰é¢„æµ‹æ­£ç¡®æ€§åˆ†ç»„
            correct = aligned_df[aligned_df['predict_correct'] == 1]
            errors = aligned_df[aligned_df['predict_correct'] == 0]
            
            distribution = {
                'correct': {
                    'mean_prob': float(correct['predicted_prob'].mean()),
                    'std_prob': float(correct['predicted_prob'].std()),
                    'min_prob': float(correct['predicted_prob'].min()),
                    'max_prob': float(correct['predicted_prob'].max())
                },
                'errors': {
                    'mean_prob': float(errors['predicted_prob'].mean()),
                    'std_prob': float(errors['predicted_prob'].std()),
                    'min_prob': float(errors['predicted_prob'].min()),
                    'max_prob': float(errors['predicted_prob'].max())
                }
            }
            
            # åˆ†ææ¦‚ç‡åŒºé—´
            prob_bins = [0.0, 0.3, 0.5, 0.7, 1.0]
            aligned_df['prob_bin'] = pd.cut(aligned_df['predicted_prob'], bins=prob_bins)
            
            bin_analysis = {}
            for bin_name, group in aligned_df.groupby('prob_bin'):
                bin_analysis[str(bin_name)] = {
                    'total': len(group),
                    'errors': len(group[group['predict_correct'] == 0]),
                    'error_rate': len(group[group['predict_correct'] == 0]) / len(group) if len(group) > 0 else 0
                }
            
            distribution['by_bin'] = bin_analysis
            
            return distribution
        except Exception as e:
            logger.error(f"åˆ†ææ¦‚ç‡åˆ†å¸ƒå¤±è´¥: {e}")
            return {}
    
    def _analyze_price_change_distribution(self, aligned_df: pd.DataFrame) -> Dict:
        """
        åˆ†ææ¶¨è·Œå¹…åˆ†å¸ƒ
        
        Args:
            aligned_df: å¯¹é½æ•°æ®
            
        Returns:
            æ¶¨è·Œå¹…ç»Ÿè®¡
        """
        try:
            # æŒ‰é¢„æµ‹æ­£ç¡®æ€§åˆ†ç»„
            correct = aligned_df[aligned_df['predict_correct'] == 1]
            errors = aligned_df[aligned_df['predict_correct'] == 0]
            
            analysis = {
                'correct': {
                    'mean_change': float(correct['actual_change'].mean()),
                    'std_change': float(correct['actual_change'].std()),
                    'abs_mean_change': float(abs(correct['actual_change']).mean())
                },
                'errors': {
                    'mean_change': float(errors['actual_change'].mean()),
                    'std_change': float(errors['actual_change'].std()),
                    'abs_mean_change': float(abs(errors['actual_change']).mean())
                }
            }
            
            return analysis
        except Exception as e:
            logger.error(f"åˆ†ææ¶¨è·Œå¹…åˆ†å¸ƒå¤±è´¥: {e}")
            return {}
    
    def identify_error_stocks(self, aligned_df: pd.DataFrame, top_n: int = 10) -> pd.DataFrame:
        """
        è¯†åˆ«è¯¯å·®æœ€å¤§çš„è‚¡ç¥¨
        
        Args:
            aligned_df: å¯¹é½æ•°æ®
            top_n: è¿”å›å‰Nåªè‚¡ç¥¨
            
        Returns:
            è¯¯å·®è‚¡ç¥¨DataFrame
        """
        try:
            if aligned_df.empty:
                return pd.DataFrame()
            
            # æ·»åŠ è¯¯å·®å¹…åº¦
            aligned_df['error_magnitude'] = abs(aligned_df['actual_change'])
            
            # æŒ‰è¯¯å·®å¹…åº¦æ’åº
            error_stocks = aligned_df[
                aligned_df['predict_correct'] == 0
            ].sort_values('error_magnitude', ascending=False).head(top_n)
            
            return error_stocks
        except Exception as e:
            logger.error(f"è¯†åˆ«è¯¯å·®è‚¡ç¥¨å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def get_feature_importance(self) -> Dict:
        """
        è·å–ç‰¹å¾é‡è¦æ€§
        
        Returns:
            ç‰¹å¾é‡è¦æ€§å­—å…¸
        """
        try:
            if self.predictor is None or self.predictor.model is None:
                logger.warning("é¢„æµ‹å™¨æˆ–æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è·å–ç‰¹å¾é‡è¦æ€§")
                return {}
            
            # è·å–ç‰¹å¾é‡è¦æ€§
            importance = self.predictor.model.get_score(importance_type='gain')
            
            # è½¬æ¢ä¸ºDataFrame
            importance_df = pd.DataFrame([
                {'feature': feat, 'importance': importance.get(f'f{i}', 0)}
                for i, feat in enumerate(self.features)
            ]).sort_values('importance', ascending=False)
            
            result = {
                'feature_importance': importance_df.to_dict('records'),
                'top_features': importance_df.head(10)['feature'].tolist(),
                'low_importance_features': importance_df.tail(5)['feature'].tolist()
            }
            
            logger.info(f"è·å–ç‰¹å¾é‡è¦æ€§æˆåŠŸï¼ŒTopç‰¹å¾: {result['top_features'][:5]}")
            return result
            
        except Exception as e:
            logger.error(f"è·å–ç‰¹å¾é‡è¦æ€§å¤±è´¥: {e}")
            return {}
    
    def analyze_error_by_threshold(self, aligned_df: pd.DataFrame) -> Dict:
        """
        åˆ†æä¸åŒé˜ˆå€¼ä¸‹çš„è¯¯å·®
        
        Args:
            aligned_df: å¯¹é½æ•°æ®
            
        Returns:
            ä¸åŒé˜ˆå€¼ä¸‹çš„è¯¯å·®åˆ†æ
        """
        try:
            if aligned_df.empty:
                return {}
            
            thresholds = np.arange(0.3, 0.6, 0.05)
            threshold_analysis = []
            
            for threshold in thresholds:
                # ä½¿ç”¨æ–°é˜ˆå€¼é‡æ–°é¢„æµ‹
                new_predictions = (aligned_df['predicted_prob'] >= threshold).astype(int)
                new_labels = aligned_df['actual_label'].values
                
                # è®¡ç®—æŒ‡æ ‡
                tp = ((new_predictions == 1) & (new_labels == 1)).sum()
                fp = ((new_predictions == 1) & (new_labels == 0)).sum()
                fn = ((new_predictions == 0) & (new_labels == 1)).sum()
                tn = ((new_predictions == 0) & (new_labels == 0)).sum()
                
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
                
                threshold_analysis.append({
                    'threshold': float(threshold),
                    'precision': float(precision),
                    'recall': float(recall),
                    'f1': float(f1),
                    'fp_count': int(fp),
                    'fn_count': int(fn)
                })
            
            result = {
                'threshold_analysis': threshold_analysis,
                'best_threshold_for_precision': min(threshold_analysis, key=lambda x: x['precision']),
                'best_threshold_for_recall': min(threshold_analysis, key=lambda x: -x['recall']),
                'best_threshold_for_f1': min(threshold_analysis, key=lambda x: -x['f1'])
            }
            
            return result
        except Exception as e:
            logger.error(f"åˆ†æé˜ˆå€¼è¯¯å·®å¤±è´¥: {e}")
            return {}
    
    def generate_error_report(self, aligned_df: pd.DataFrame, error_analysis: Dict = None) -> str:
        """
        ç”Ÿæˆè¯¯å·®åˆ†ææŠ¥å‘Š
        
        Args:
            aligned_df: å¯¹é½æ•°æ®
            error_analysis: è¯¯å·®åˆ†æç»“æœ
            
        Returns:
            Markdownæ ¼å¼çš„æŠ¥å‘Š
        """
        if error_analysis is None:
            error_analysis = self.analyze_errors(aligned_df)
        
        report = []
        report.append("# è¯¯å·®æº¯æºåˆ†ææŠ¥å‘Š\n")
        report.append(f"ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # 1. æ€»ä½“è¯¯å·®ç»Ÿè®¡
        report.append("## 1. æ€»ä½“è¯¯å·®ç»Ÿè®¡\n")
        report.append(f"- æ€»æ ·æœ¬æ•°: {error_analysis.get('total_samples', 0)}\n")
        report.append(f"- é”™è¯¯æ•°é‡: {error_analysis.get('error_count', 0)}\n")
        report.append(f"- æ­£ç¡®æ•°é‡: {error_analysis.get('correct_count', 0)}\n")
        report.append(f"- è¯¯å·®ç‡: {error_analysis.get('error_rate', 0)*100:.2f}%\n")
        
        # 2. è¯¯å·®ç±»å‹åˆ†æ
        report.append("\n## 2. è¯¯å·®ç±»å‹åˆ†æ\n")
        report.append(f"- å‡æ­£ä¾‹(é¢„æµ‹ä¸Šæ¶¨å®é™…ä¸‹è·Œ): {error_analysis.get('false_positive_count', 0)} ({error_analysis.get('false_positive_rate', 0)*100:.2f}%)\n")
        report.append(f"- å‡è´Ÿä¾‹(é¢„æµ‹ä¸‹è·Œå®é™…ä¸Šæ¶¨): {error_analysis.get('false_negative_count', 0)} ({error_analysis.get('false_negative_rate', 0)*100:.2f}%)\n")
        
        # 3. æ¦‚ç‡åˆ†å¸ƒ
        if 'probability_distribution' in error_analysis:
            report.append("\n## 3. é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ\n")
            prob_dist = error_analysis['probability_distribution']
            report.append("### æ­£ç¡®é¢„æµ‹\n")
            report.append(f"- å¹³å‡æ¦‚ç‡: {prob_dist['correct']['mean_prob']:.4f}\n")
            report.append(f"- æ ‡å‡†å·®: {prob_dist['correct']['std_prob']:.4f}\n")
            report.append("### é”™è¯¯é¢„æµ‹\n")
            report.append(f"- å¹³å‡æ¦‚ç‡: {prob_dist['errors']['mean_prob']:.4f}\n")
            report.append(f"- æ ‡å‡†å·®: {prob_dist['errors']['std_prob']:.4f}\n")
        
        # 4. è¯¯å·®æœ€å¤§çš„è‚¡ç¥¨
        report.append("\n## 4. è¯¯å·®æœ€å¤§çš„è‚¡ç¥¨\n")
        error_stocks = self.identify_error_stocks(aligned_df, top_n=5)
        if not error_stocks.empty:
            for _, row in error_stocks.iterrows():
                report.append(f"- {row['ts_code']}: é¢„æµ‹={'ä¸Šæ¶¨' if row['predicted_label']==1 else 'ä¸‹è·Œ'}, "
                            f"å®é™…={'ä¸Šæ¶¨' if row['actual_label']==1 else 'ä¸‹è·Œ'}, "
                            f"æ¶¨è·Œå¹…={row['actual_change']*100:.2f}%\n")
        else:
            report.append("æ— è¯¯å·®æ•°æ®\n")
        
        # 5. è°ƒæ•´å»ºè®®
        report.append("\n## 5. è°ƒæ•´å»ºè®®\n")
        
        fp_rate = error_analysis.get('false_positive_rate', 0)
        fn_rate = error_analysis.get('false_negative_rate', 0)
        
        if fp_rate > 0.3:
            report.append("- âš ï¸ å‡æ­£ä¾‹è¿‡å¤šï¼Œå»ºè®®æé«˜åˆ†ç±»é˜ˆå€¼æˆ–è°ƒæ•´scale_pos_weight\n")
        if fn_rate > 0.2:
            report.append("- âš ï¸ å‡è´Ÿä¾‹è¿‡å¤šï¼Œå»ºè®®é™ä½åˆ†ç±»é˜ˆå€¼æˆ–ä¼˜åŒ–ç‰¹å¾\n")
        
        if self.predictor:
            importance = self.get_feature_importance()
            if 'low_importance_features' in importance:
                report.append(f"- ğŸ’¡ è€ƒè™‘ç§»é™¤é‡è¦æ€§è¾ƒä½çš„ç‰¹å¾: {', '.join(importance['low_importance_features'])}\n")
        
        return ''.join(report)
    
    def save_error_report(self, report: str, filename: str = None):
        """
        ä¿å­˜è¯¯å·®æŠ¥å‘Š
        
        Args:
            report: æŠ¥å‘Šå†…å®¹
            filename: æ–‡ä»¶å
        """
        try:
            workspace_path = os.getenv("COZE_WORKSPACE_PATH", "/workspace/projects")
            if filename is None:
                timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
                filename = f"error_report_{timestamp}.md"
            
            save_path = os.path.join(workspace_path, "assets/logs", filename)
            
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report)
            
            logger.info(f"ä¿å­˜è¯¯å·®æŠ¥å‘ŠæˆåŠŸ: {save_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜è¯¯å·®æŠ¥å‘Šå¤±è´¥: {e}")


def test_error_tracker():
    """æµ‹è¯•è¯¯å·®è¿½è¸ªå™¨"""
    tracker = ErrorTracker()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print("\n=== æµ‹è¯•è¯¯å·®åˆ†æ ===")
    np.random.seed(42)
    
    aligned_df = pd.DataFrame({
        'ts_code': [f'60000{i}.SH' for i in range(20)],
        'predict_date': ['20241201'] * 20,
        'actual_date': ['20241206'] * 20,
        'predicted_label': np.random.randint(0, 2, 20),
        'predicted_prob': np.random.random(20),
        'actual_label': np.random.randint(0, 2, 20),
        'actual_change': np.random.randn(20) * 0.05,
        'predict_correct': [1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0]
    })
    
    # è¯¯å·®åˆ†æ
    error_analysis = tracker.analyze_errors(aligned_df)
    print(f"\nè¯¯å·®åˆ†æ:\n{error_analysis}")
    
    # è¯†åˆ«è¯¯å·®è‚¡ç¥¨
    error_stocks = tracker.identify_error_stocks(aligned_df, top_n=5)
    print(f"\nè¯¯å·®æœ€å¤§çš„è‚¡ç¥¨:\n{error_stocks}")
    
    # é˜ˆå€¼åˆ†æ
    threshold_analysis = tracker.analyze_error_by_threshold(aligned_df)
    print(f"\næœ€ä¼˜é˜ˆå€¼åˆ†æ:\n{threshold_analysis}")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = tracker.generate_error_report(aligned_df, error_analysis)
    print(f"\nè¯¯å·®æŠ¥å‘Š:\n{report}")


if __name__ == '__main__':
    test_error_tracker()
